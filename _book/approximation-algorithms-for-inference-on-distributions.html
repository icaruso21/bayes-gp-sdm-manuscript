<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Approximation algorithms for inference on distributions | Bayesian Inference for Species Distribution Modelling with Gaussian Processes</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Approximation algorithms for inference on distributions | Bayesian Inference for Species Distribution Modelling with Gaussian Processes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Approximation algorithms for inference on distributions | Bayesian Inference for Species Distribution Modelling with Gaussian Processes" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  



<meta name="date" content="2021-04-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="modern-bayesian-inference-with-stan.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Probability and Bayes’ theorem</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.1</b> Random Variables</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.2</b> Cumulative Distribution Functions</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#probability-mass-and-density-functions"><i class="fa fa-check"></i><b>1.3</b> Probability Mass and Density Functions</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#expectation"><i class="fa fa-check"></i><b>1.4</b> Expectation</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#conditional-probability"><i class="fa fa-check"></i><b>1.5</b> Conditional Probability</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#bayes-theorem-for-discrete-values"><i class="fa fa-check"></i><b>1.6</b> Bayes’ Theorem for Discrete Values</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#bayes-theorem-for-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Bayes’ Theorem for Probability Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html"><i class="fa fa-check"></i><b>2</b> Approximation algorithms for inference on distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>2.1</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#markov-chains"><i class="fa fa-check"></i><b>2.1.1</b> Markov Chains</a></li>
<li class="chapter" data-level="2.1.2" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#monte-carlo-approximation"><i class="fa fa-check"></i><b>2.1.2</b> Monte Carlo Approximation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>2.2</b> Metropolis-Hastings Monte Carlo</a></li>
<li class="chapter" data-level="2.3" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>2.3</b> Hamiltonian Monte Carlo</a></li>
<li class="chapter" data-level="2.4" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#no-u-turn-sampling"><i class="fa fa-check"></i><b>2.4</b> No-U-Turn Sampling</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modern-bayesian-inference-with-stan.html"><a href="modern-bayesian-inference-with-stan.html"><i class="fa fa-check"></i><b>3</b> Modern Bayesian inference with STAN</a></li>
<li class="chapter" data-level="4" data-path="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html"><a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html"><i class="fa fa-check"></i><b>4</b> Bayesian applications in Biology: Hybrid species distribution modelling with gaussian processes</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference for Species Distribution Modelling with Gaussian Processes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="approximation-algorithms-for-inference-on-distributions" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Approximation algorithms for inference on distributions</h1>
<p>When performing Bayesian Inference, approximation algorithms are necessary for updating the relative probability of parameters of interest in a computationally feasible manner. In modelling scenarios with multiple parameters forming a multidimensional parameter space, evaluating parameters at even a relatively small number of possible vlaues becomes computationally intractible. This problem is further exaggerated in the evaluation of hierarchical models, which are of particular interest in many application domains including biology, economics, chemistry, and physics. Here, we discuss markov chain monte carlo based sampling algorithms, which allow for efficiently sampling from approximations of probability distributions.</p>
<div id="markov-chain-monte-carlo" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Markov Chain Monte Carlo</h2>
<p>Markov chain monte carlo (MCMC) provides an efficient methodology for sampling from the posterior curve to perform a Bayesian update. Before approaching algorithms for performing MCMC, we must first introduce two computational techniques: markov chains and monte carlo approximation.</p>
<div id="markov-chains" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Markov Chains</h3>
<p>A markov chain is a stochastic model expressing a sequence of possible states in which the probability of each state, <span class="math inline">\(X_{i-1}\)</span>, depends only on the value attained in the previous state, <span class="math inline">\(X_{i-1}\)</span>. A markov chain has several properties which are essential for its application in Bayesian statistics. Namely, each state in the chain depends only on the previous one; therefore, the markov chain preserves the assumed dependence between samples from our posterior distribution. Additionally, this localized dependence lends the markov chain another attractive feature in that it is effectively memoryless. Definition <a href="approximation-algorithms-for-inference-on-distributions.html#def:markov">2.1</a> formalizes the requirements for a markov chain <span class="citation">(<a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html#ref-Wasserman2004" role="doc-biblioref">Wasserman 2004</a>)</span>.</p>

<div class="definition">
<span id="def:markov" class="definition"><strong>Definition 2.1  (Markov Chain)  </strong></span><em>A discrete sequence of random variables {X_{0},X_{1},…,X_{i}} is a Markov chain iff it satisfies the Markov property. That is for all <span class="math inline">\(i\)</span> and <span class="math inline">\(x \in X\)</span>:</em>
<span class="math display">\[P(X_{i}=x|X_{0},...X_{i-1})=P(X_{i}=x|X_{i-1})\]</span>
</div>
<p>A markov chain is often represented as a directed graph where states in the sequence of random variables are represented as vertices and edges represent possible paths between states whose weights are the probabilities of edges being traversed.</p>
</div>
<div id="monte-carlo-approximation" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Monte Carlo Approximation</h3>
<p>Monte Carlo approximation provides a convient method for approximately computing quantities of interest. This schema will allow for drawing samples from arbitrarily complex probability distributions. The basic example, known as monte carlo integration, evaluates an integral using monte carlo approximation <span class="citation">(<a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html#ref-Wasserman2004" role="doc-biblioref">Wasserman 2004</a>)</span>. If we want to evaulate an integral for some function <span class="math inline">\(f(x)\)</span>:
<span class="math display">\[I=\int_{a}^{b}f(x)dx\]</span>
we can approximate <span class="math inline">\(I\)</span> using monte carlo approximation. <span class="math inline">\(f(x)\)</span> can be alternatively expressed as two functions, <span class="math inline">\(h(x)\)</span> and <span class="math inline">\(w(x)\)</span>, where <span class="math inline">\(h(x)=\frac{1}{a-b}\)</span> and <span class="math inline">\(w(x)=f(x)(b-a)\)</span> as follows:</p>
<p><span class="math display">\[I=\int_{a}^{b}f(x)dx=\int_{a}^{b}w(x)h(x)dx\]</span>
Conveniently, <span class="math inline">\(h\)</span> is a pdf of a uniform r.v. <span class="math inline">\(X\)</span> over <span class="math inline">\([a,b]\)</span>, which means that <span class="math inline">\(I\)</span> can be rewritten in terms of expectation as
<span class="math display">\[I=E_{f}(w(X))\]</span>
In conjunction with the law of large numbers, this means that if we generate a sequence of random variables from a uniform distribution <span class="math inline">\(X_{0},...,X_{N}\sim unif(a,b)\)</span> then the standard Monte Carlo integration method asserts</p>
<p><span class="math display">\[\hat{I}\equiv\frac{1}{N}\sum_{i=0}^{N}w(X_{i})\rightarrow E(w(X))=I \textrm{ .}\]</span></p>
<p>In other words, <span class="math inline">\(\hat{I}\)</span> approaches <span class="math inline">\(I\)</span> as <span class="math inline">\(N\)</span> grows sufficiently large. Generally, constructing a Markov chain for evaluation with Monte Carlo will maintain the inherent, presumed dependence between samples of a sequence of random variables while facilitating the random sampling of extremely complex probability distributions. As we will expand on in the following sections, this technique is particularly useful because will work for models with non-normal posteriors, including hierarchical models.</p>
</div>
</div>
<div id="metropolis-hastings-monte-carlo" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Metropolis-Hastings Monte Carlo</h2>
</div>
<div id="hamiltonian-monte-carlo" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Hamiltonian Monte Carlo</h2>
</div>
<div id="no-u-turn-sampling" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> No-U-Turn Sampling</h2>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modern-bayesian-inference-with-stan.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["manuscript.pdf", "manuscript.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
