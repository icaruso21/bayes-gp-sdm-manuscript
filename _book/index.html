<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Bayesian Inference for Species Distribution Modelling with Gaussian Processes</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Bayesian Inference for Species Distribution Modelling with Gaussian Processes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Inference for Species Distribution Modelling with Gaussian Processes" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  



<meta name="date" content="2021-03-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="approximation-algorithms-for-inference-on-complex-systems.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Probability, Bayes’ theorem, and Bayesian inference</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.1</b> Random Variables</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.2</b> Cumulative Distribution Functions</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#probability-mass-and-density-functions"><i class="fa fa-check"></i><b>1.3</b> Probability Mass and Density Functions</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#bayes-theorem"><i class="fa fa-check"></i><b>1.4</b> Bayes’ Theorem</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#bayesian-inference"><i class="fa fa-check"></i><b>1.5</b> Bayesian Inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="approximation-algorithms-for-inference-on-complex-systems.html"><a href="approximation-algorithms-for-inference-on-complex-systems.html"><i class="fa fa-check"></i><b>2</b> Approximation algorithms for inference on complex systems</a></li>
<li class="chapter" data-level="3" data-path="modern-bayesian-inference-with-stan.html"><a href="modern-bayesian-inference-with-stan.html"><i class="fa fa-check"></i><b>3</b> Modern Bayesian inference with STAN</a></li>
<li class="chapter" data-level="4" data-path="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html"><a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html"><i class="fa fa-check"></i><b>4</b> Bayesian applications in Biology: Hybrid species distribution modelling with gaussian processes</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference for Species Distribution Modelling with Gaussian Processes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Bayesian Inference for Species Distribution Modelling with Gaussian Processes</h1>
<h2 class="subtitle"><em>Isaac William Caruso</em></h2>
<p class="author"><em><div class="line-block">Amherst College<br />
Department of Computer Science</div></em></p>
<p class="date"><em>2021-03-30</em></p>
</div>
<div id="probability-bayes-theorem-and-bayesian-inference" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Probability, Bayes’ theorem, and Bayesian inference</h1>
<p>The objective of this chapter is to provide an introduction and brief summary of the probability terms and concepts necessary for understanding and approaching Bayesian inference.
Simply put, Bayesian inference is a statistical technique for iteratively updating a probability quantity of interest upon the observation of data.
Specifically, the statistical technique Bayesian inference relies on is known as Bayes’ theorem and the probability quantity of interest is most often a probability density function, though in introductory examples is oftentimes a single value.
Before embarking on this exposition of Bayesian statistics, one must first gain a basic appreciation for a few key elements of probability theory which will provide us with the foundational terms to discuss Bayesian inference and its applications in the broad field of machine learning—random variables and probability functions.</p>
<div id="random-variables" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Random Variables</h2>
<p>Imagine for a moment you are tossing a coin.
There are many experiments you could perform by tossing a coin, but for the sake of simplicity let us consider our quantity of interest to be the number of times our coin lands on a tails.
It is clear that the number of tails, the outcome of our experiment, is dependent on the eventual realization of some random (stochastic) process, or at the very least not clearly deterministic.
In countless scenarios like the coin toss experiment, random variables provide a convenient and uniform terminology for discussing sample spaces and their connection to some sequence of observations that is the data.
In essence, a random variable is a variable whose value is dependent on the outcome(s) of a stochastic phenomenon.
A random variable is commonly expressed as <span class="math inline">\(X\)</span>, and a realized value of <span class="math inline">\(X\)</span> is <span class="math inline">\(x\)</span>.</p>
<p>In the canonical example of tossing a coin, where the data is some sequence of coin tosses <span class="math inline">\([H, T, T, …]\)</span>, we could define a random variable <span class="math inline">\(X\)</span> to be the number of tails.
In this toy example, as noted above, the realized value of <span class="math inline">\(X\)</span>, is dependent on the outcome(s) of a random event—a coin toss.
In the case of tossing a coin twice consecutively, <span class="math inline">\(X\)</span>—the number of tails observations—has three possible realized states, <span class="math inline">\(x\)</span>, depending on the outcome of this stochastic experiment: <span class="math inline">\(x = 0\)</span>, <span class="math inline">\(x = 1\)</span>, or <span class="math inline">\(x = 2\)</span>.
Additionally, assuming the coin is a fair coin the probability that our random variable <span class="math inline">\(X\)</span> is realized as <span class="math inline">\(x\)</span>, some real number of tails, can be assigned to each possible overall outcome of this coin toss experiment as such:</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:table1">Table 1.1: </span>P(X = x) for two tosses
</caption>
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
P(X = x)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.25
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.50
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.25
</td>
</tr>
</tbody>
</table>
<p>In reality, this is an example of a specific type of random variable known as a discrete random variable.
Discrete random variables can, as their name implies, only assume discrete values.
To the contrary, continuous random variables and mixed random variables are useful for describing sample spaces with continuous and mixed outcomes, respectively.
For example, a continuous random variable may be used to describe an experiment measuring blossoming heights of flowers, where the data is a sequence of observations of heights at which different flowers blossomed.
In this case, the outcome of our blossoming experiment is an infinite number of real values that is properly represented in a continuous random variable.</p>
</div>
<div id="cumulative-distribution-functions" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Cumulative Distribution Functions</h2>
<p>The concept of a distribution follows intuitively from our previous discussion of random variables.
In the previous example we represented the probability of various outcomes of a coin toss experiment in a tabular format.
Another way to represent this distribution of probabilities is as a cumulative distribution function:</p>

<div class="definition">
<p><span id="def:cdf" class="definition"><strong>Definition 1.1  (Cumulative distribution function ‘CDF’)  </strong></span><em>The cumulative distribution function is defined as a function where <span class="math inline">\(F_{X}\in[0, 1]\)</span>:</em></p>
<span class="math display">\[F_{X}(x)=P(X≤x)\]</span>
</div>
<p>As the name implies, the cumulative distribution function simply represents the probability that a random variable <span class="math inline">\(X\)</span> is realized to be less than or equal to <span class="math inline">\(x\)</span> for each possible input value of <span class="math inline">\(x\)</span>.
Figure <a href="index.html#fig:cdfimg">1.1</a> depicts a graphical representation of the CDF for our coin tossing experiment.</p>
<div class="figure" style="text-align: center"><span id="fig:cdfimg"></span>
<img src="images/cdf.png" alt="CDF for tossing a coin twice (from All of Stats, should I make my own?)" width="70%" />
<p class="caption">
Figure 1.1: CDF for tossing a coin twice (from All of Stats, should I make my own?)
</p>
</div>
<p>Here, for every value of <span class="math inline">\(x\)</span>—the number of tails in two coin tosses—the probability that <span class="math inline">\(X\)</span> is equal to or less than this value is represented.
In this discrete example, we see that our CDF is represented by several non-decreasing discrete lines defined for all <span class="math inline">\(x\)</span>.
In the example of a continuous random variable, this function is a continuous, non-decreasing distribution also defined for all <span class="math inline">\(x\)</span>.</p>
</div>
<div id="probability-mass-and-density-functions" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Probability Mass and Density Functions</h2>
<p>Now that we have a basic understanding of random variables and their association with the CDF, we can begin discussing probability functions for both continuous and discrete random variables.
Broadly speaking, the probability mass function and probability density function are two methods for calculating a probability over a sample space, which find their use with discrete and continuous random variables respectively.</p>
<p>In the discrete setting, a probability mass function yields the probability of an outcome for every possible outcome in any given discrete schema.
To reiterate the previous discussion of discrete versus continuous random variables, a random variable <span class="math inline">\(X\)</span> is considered discrete “if it takes countably many values <span class="math inline">\(\{x_{1}, x_{2}, …\}\)</span>” <span class="citation">(<a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html#ref-Wasserman2004" role="doc-biblioref">Wasserman 2004</a>)</span>.
In this case, a definition for the probability mass function follows:</p>

<div class="definition">
<p><span id="def:pmf" class="definition"><strong>Definition 1.2  (Probability mass function ‘PMF’)  </strong></span><em>The probability function for a discrete random variable <span class="math inline">\(X\)</span>—the probability mass function for <span class="math inline">\(X\)</span>—is defined as a function</em></p>
<span class="math display">\[F_{X}(x)=P(X=x)\]</span>
</div>
<p>Here, the PMF has a few key attributes.
Namely <span class="math inline">\(P(X=x)&gt;0\)</span> for every <span class="math inline">\(x\)</span> in the sample space of <span class="math inline">\(X (x \in S_{X})\)</span>, and <span class="math inline">\(\sum_{x \in S_{X}}f(x)=1\)</span>.
With these features in mind, the probability mass function of <span class="math inline">\(X\)</span> follows logically from the cumulative distribution function of <span class="math inline">\(X\)</span> insofar as the CDF is the sum of the PMF for all <span class="math inline">\(x_{i}≤x\)</span>.
This relationship in the discrete scenario between the PMF and CDF can be expressed formally as: <span class="math display">\[F_{X}(x)=P(X≤x)=\sum_{x_i≤x}f_{X}(x_{i})\]</span></p>
<p>In the continuous setting where these attributes no longer hold, a probability density function (PDF) is instead used to represent probability of <span class="math inline">\(x\)</span> for all <span class="math inline">\(x \in S_X\)</span>.
In the case where the random variable <span class="math inline">\(X\)</span> is continuous, a PDF is defined as follows:</p>

<div class="definition">
<p><span id="def:pdf" class="definition"><strong>Definition 1.3  (Probability density function ‘PDF’)  </strong></span><em>The probability function for a continuous random variable <span class="math inline">\(X\)</span>—the probability density function for <span class="math inline">\(X\)</span>—is defined as a function <span class="math inline">\(f(x)\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are two real numbers and every <span class="math inline">\(a ≤ b\)</span>, so</em></p>
<span class="math display">\[P(a&lt;X&lt;b)=\int_{a}^{b}f_X(x)dx\]</span>
</div>
<p>In other words, the probability that the realized value <span class="math inline">\(x\)</span> of our continuous random variable <span class="math inline">\(X\)</span> is between two numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is equal to the integral of the probability density function of <span class="math inline">\(x\)</span> from <span class="math inline">\(x = a\)</span> to <span class="math inline">\(x = b\)</span>.
This formalization of the PDF <span class="math inline">\(f_X(x)\)</span> allows a natural comparison to be drawn to the CDF <span class="math inline">\(F_X(x)\)</span> of a continuous random variable <span class="math inline">\(X\)</span>, that is:</p>
<p><span class="math display">\[F_X(x)=\int_{-\infty}^{x}f_X(x)dx\]</span></p>
<p>Specifically, this implies that <span class="math inline">\(F&#39;_X(x)=f_X(x)\)</span> for all differentiable points in <span class="math inline">\(F_X\)</span>.
In plain English this signifies that the derivative of the CDF is the PDF, in all cases involving a continuous random variable.</p>
<p>Probability density and mass functions come in a wide variety of forms, each with their own properties and methodologies for defining their shape and characteristics.
Some are useful for defining binary data, others are useful for defining arbitrarily large sample spaces with any number of localized concentrations of density.
Still others boast properties, such as being closed under conditioning and marginalization, that make them useful for specific statistical applications.
Figure <a href="index.html#fig:distributions">1.2</a> displays some of the most common continuous and discrete probability distributions.</p>
<div class="figure" style="text-align: center"><span id="fig:distributions"></span>
<img src="images/distributions.png" alt="Common probability distributions" width="70%" />
<p class="caption">
Figure 1.2: Common probability distributions
</p>
</div>
<p>More text now <span class="citation">(<a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html#ref-Kotta2019" role="doc-biblioref">Kotta et al. 2019</a>)</span></p>
</div>
<div id="bayes-theorem" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Bayes’ Theorem</h2>
</div>
<div id="bayesian-inference" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Bayesian Inference</h2>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="approximation-algorithms-for-inference-on-complex-systems.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["manuscript.pdf", "manuscript.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
