<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Bayesian Inference for Species Distribution Modelling with Gaussian Processes</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Bayesian Inference for Species Distribution Modelling with Gaussian Processes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bayesian Inference for Species Distribution Modelling with Gaussian Processes" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  



<meta name="date" content="2021-04-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="approximation-algorithms-for-inference-on-distributions.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Probability and Bayes’ theorem</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.1</b> Random Variables</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>1.2</b> Cumulative Distribution Functions</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#probability-mass-and-density-functions"><i class="fa fa-check"></i><b>1.3</b> Probability Mass and Density Functions</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#expectation"><i class="fa fa-check"></i><b>1.4</b> Expectation</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#conditional-probability"><i class="fa fa-check"></i><b>1.5</b> Conditional Probability</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#bayes-theorem-for-discrete-values"><i class="fa fa-check"></i><b>1.6</b> Bayes’ Theorem for Discrete Values</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#bayes-theorem-for-probability-distributions"><i class="fa fa-check"></i><b>1.7</b> Bayes’ Theorem for Probability Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html"><i class="fa fa-check"></i><b>2</b> Approximation algorithms for inference on distributions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>2.1</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#markov-chains"><i class="fa fa-check"></i><b>2.1.1</b> Markov Chains</a></li>
<li class="chapter" data-level="2.1.2" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#monte-carlo-approximation"><i class="fa fa-check"></i><b>2.1.2</b> Monte Carlo Approximation</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>2.2</b> Metropolis-Hastings Monte Carlo</a></li>
<li class="chapter" data-level="2.3" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>2.3</b> Hamiltonian Monte Carlo</a></li>
<li class="chapter" data-level="2.4" data-path="approximation-algorithms-for-inference-on-distributions.html"><a href="approximation-algorithms-for-inference-on-distributions.html#no-u-turn-sampling"><i class="fa fa-check"></i><b>2.4</b> No-U-Turn Sampling</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modern-bayesian-inference-with-stan.html"><a href="modern-bayesian-inference-with-stan.html"><i class="fa fa-check"></i><b>3</b> Modern Bayesian inference with STAN</a></li>
<li class="chapter" data-level="4" data-path="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html"><a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html"><i class="fa fa-check"></i><b>4</b> Bayesian applications in Biology: Hybrid species distribution modelling with gaussian processes</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian Inference for Species Distribution Modelling with Gaussian Processes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Bayesian Inference for Species Distribution Modelling with Gaussian Processes</h1>
<h2 class="subtitle"><em>Isaac William Caruso</em></h2>
<p class="author"><em><div class="line-block">Amherst College<br />
Department of Computer Science</div></em></p>
<p class="date"><em>2021-04-07</em></p>
</div>
<div id="probability-and-bayes-theorem" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Probability and Bayes’ theorem</h1>
<p>This chapter provides an introduction to the probability concepts necessary to understand Bayesian inference.
Simply put, Bayesian inference is a statistical technique for estimating a quantity of interest upon the observation of data.
Before embarking on an exposition of Bayesian statistics, we must first gain a basic understanding of a few key elements of probability theory which are needed to introduce Bayesian inference and its applications in the broad field of machine learning—random variables, cumulative distribution functions, probability functions/ distributions, expected value, and conditional probability.
This chapter will then conclude by introducing Bayes’ theorem and Bayesian inference, and demonstrating the steps for performing a simple Bayesian update.</p>
<div id="random-variables" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Random Variables</h2>
<p>Imagine for a moment you are tossing a fair coin.
There are many experiments you could perform by tossing a coin, but let us consider our quantity of interest to be the fraction of times our coin lands on a tails.
It is clear that the number of tails, the outcome of our experiment, is dependent on the eventual realization of some random process.
A random variable <span class="math inline">\(X\)</span> is a variable whose value is dependent on the outcome(s) of a stochastic phenomenon.
The realized value of <span class="math inline">\(X\)</span> is denoted as <span class="math inline">\(x\)</span>.</p>
<p>In the example of tossing a coin, where the data is a sequence of coin tosses, e.g., <span class="math inline">\([H, T, T, …]\)</span>, we define the random variable <span class="math inline">\(X\)</span> to be the number of tails.
If we toss the coin twice, <span class="math inline">\(X\)</span> has three possible realized states, <span class="math inline">\(x\)</span>, depending on the outcome of this stochastic experiment: <span class="math inline">\(x = 0\)</span>, <span class="math inline">\(x = 1\)</span>, or <span class="math inline">\(x = 2\)</span>.
Table <a href="#tab:rv"><strong>??</strong></a> shows the probability that our random variable <span class="math inline">\(X\)</span> takes value <span class="math inline">\(x\)</span>, some real number of tails.</p>
<p>The r.v. <span class="math inline">\(X\)</span> is is an example of a <em>discrete random variable</em>.
Discrete random variables can only assume discrete values.
To the contrary, continuous random variables are useful for describing continuous sample spaces.
For example, a continuous random variable may be used to represent the outcome of an experiment measuring blossoming heights of flowers, where the data is a sequence of observations of heights at which different flowers blossomed.
In this case, the outcome of our blossoming experiment can be any of an infinite number of real values, thus is properly modeled by a continuous random variable.</p>
</div>
<div id="cumulative-distribution-functions" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Cumulative Distribution Functions</h2>
<p>In the previous example we represented the probability of various outcomes of a coin toss experiment in a tabular format.
Another way to represent this set of probabilities is as a <em>cumulative distribution function</em>.</p>

<div class="definition">
<span id="def:cdf" class="definition"><strong>Definition 1.1  (Cumulative distribution function ‘CDF’)  </strong></span><em>The cumulative distribution function is defined as a function where <span class="math inline">\(F_{X}\in[0, 1]\)</span>:</em>
<span class="math display">\[F_{X}(x)=P(X≤x)\]</span>
</div>
<p>The cumulative distribution function <span class="math inline">\(F_{X}(x)\)</span> simply represents the probability that a random variable <span class="math inline">\(X\)</span> takes a value less than or equal to <span class="math inline">\(x\)</span> for each possible input value of <span class="math inline">\(x\)</span>.
Figure <a href="index.html#fig:cdfimg">1.1</a> depicts a graphical representation of the CDF for our coin tossing experiment.</p>
<div class="figure" style="text-align: center"><span id="fig:cdfimg"></span>
<img src="images/cdf.png" alt="CDF for tossing a coin twice [@Wasserman2004]" width="70%" />
<p class="caption">
Figure 1.1: CDF for tossing a coin twice <span class="citation">(<a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html#ref-Wasserman2004" role="doc-biblioref">Wasserman 2004</a>)</span>
</p>
</div>
<p>Here, for every value of <span class="math inline">\(x\)</span>—the number of tails in two coin tosses—the probability that <span class="math inline">\(X\)</span> is equal to or less than this value is represented.
In this discrete example, we see that our CDF is represented by several non-decreasing discrete lines defined for all <span class="math inline">\(x\)</span>.
In the example of a continuous random variable, this function is a continuous, non-decreasing distribution also defined for all <span class="math inline">\(x\)</span>.</p>
</div>
<div id="probability-mass-and-density-functions" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Probability Mass and Density Functions</h2>
<p>The probability mass function and probability density function are two methods for calculating a probability over a sample space, which find their use with discrete and continuous random variables respectively.</p>
<p>In the discrete setting, the probability mass function for a random variable <span class="math inline">\(X\)</span> yields the probability that <span class="math inline">\(X\)</span> takes a value for every possible value that <span class="math inline">\(X\)</span> can take.</p>

<div class="definition">
<span id="def:pmf" class="definition"><strong>Definition 1.2  (Probability mass function ‘PMF’)  </strong></span><em>The probability function for a discrete random variable <span class="math inline">\(X\)</span>—the probability mass function for <span class="math inline">\(X\)</span>—is defined as a function</em>
<span class="math display">\[f_{X}(x)=P(X=x)\]</span>
</div>
<p>Here, the PMF has a few key attributes.
Namely <span class="math inline">\(P(X=x)&gt;0\)</span> for every <span class="math inline">\(x\)</span> in the sample space <span class="math inline">\(S_{X}\)</span> of <span class="math inline">\(X\)</span>, and <span class="math inline">\(\sum_{x \in S_{X}}f_{X}(x)=1\)</span>.
With these features in mind, the probability mass function of <span class="math inline">\(X\)</span> follows logically from the cumulative distribution function of <span class="math inline">\(X\)</span> insofar as the CDF is the sum of the PMF for all <span class="math inline">\(x_{i}≤x\)</span>.
This relationship in the discrete scenario between the PMF and CDF can be expressed formally as: <span class="math display">\[F_{X}(x)=P(X≤x)=\sum_{x_i≤x}f_{X}(x_{i}).\]</span></p>
<p>In the case where the random variable <span class="math inline">\(X\)</span> is continuous, its PDF is defined as follows,</p>

<div class="definition">
<span id="def:pdf" class="definition"><strong>Definition 1.3  (Probability density function ‘PDF’)  </strong></span><em>The probability function for a continuous random variable <span class="math inline">\(X\)</span>—the probability density function for <span class="math inline">\(X\)</span>—is defined as a function <span class="math inline">\(f(x)\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are two real numbers such that <span class="math inline">\(a ≤ b\)</span>, so</em>
<span class="math display">\[P(a&lt;X&lt;b)=\int_{a}^{b}f_X(x)dx.\]</span>
</div>
<p>In other words, the probability that the realized value <span class="math inline">\(x\)</span> of our continuous random variable <span class="math inline">\(X\)</span> is between two numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is equal to the integral of the probability density function of <span class="math inline">\(x\)</span> from <span class="math inline">\(x = a\)</span> to <span class="math inline">\(x = b\)</span>.
This formalization of the PDF <span class="math inline">\(f_X(x)\)</span> allows a natural comparison with the CDF <span class="math inline">\(F_X(x)\)</span> of <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[F_X(x)=\int_{-\infty}^{x}f_X(x)dx.\]</span></p>
<p>Specifically, this implies that <span class="math inline">\(F&#39;_X(x)=f_X(x)\)</span> for all differentiable points of <span class="math inline">\(F_X\)</span>.
In plain English this signifies that the derivative of the CDF is the PDF.</p>
</div>
<div id="expectation" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Expectation</h2>
<p>One of the final core statistical concepts necessary to approach Bayesian statistics on sure footing is the idea of expectation or expected value.</p>

<div class="definition">
<p><span id="def:e" class="definition"><strong>Definition 1.4  (Expectation)  </strong></span><em>The expectation or expected value of a random variable X is</em>
<span class="math display">\[E(X)=\begin{cases}\sum_{x}xf_{X}(x) &amp; \textrm{if }X\textrm{ is discrete}\\\int{xf_{X}(x)dx} &amp; \textrm{if }X\textrm{ is continuous}\end{cases}\]</span></p>
</div>
<p>To return to a familiar example, consider a random variable <span class="math inline">\(X\)</span> to represent the number of tails in 6 coin tosses. Figure <a href="index.html#fig:binom">1.2</a> depicts the PMF for <span class="math inline">\(X\)</span> using the <em>binomial distribution</em> <span class="math inline">\(B(6,0.5)\)</span> which represents the probability of observing a specific number of successes in a success-failure experiment.</p>
<div class="figure" style="text-align: center"><span id="fig:binom"></span>
<img src="manuscript_files/figure-html/binom-1.png" alt="The binomial probability mass function for the 6 trial coin toss experiment" width="50%" />
<p class="caption">
Figure 1.2: The binomial probability mass function for the 6 trial coin toss experiment
</p>
</div>
<p>In this case, the x-axis represents each possible outcome <span class="math inline">\(x\)</span> and the y-axis is the probability of that outcome. Table <a href="#tab:bino"><strong>??</strong></a> presents the value of the binomial PMF for every <span class="math inline">\(x \in X\)</span>.</p>
<p>Computing <span class="math inline">\(EX\)</span> given the values in this table is demonstrated using the discrete case of Definition <a href="index.html#def:e">1.4</a> in Example <a href="index.html#exm:coin">1.1</a>.</p>

<div class="example">
<span id="exm:coin" class="example"><strong>Example 1.1  </strong></span>The expected value of <span class="math inline">\(X \sim B(6, 0.5)\)</span>,
<span class="math display">\[\begin{split}
E(X) &amp; = \sum_{x}xf(x) \\
  &amp; =  (0 \times 0.16)+(1 \times 0.094)+(2 \times 0.234)+(3 \times 0.312)\\
  &amp; \;\;\;\; +(4 \times 0.234)+(5 \times 0.094)+(6 \times 0.016)\\
  &amp; = 3
\end{split}\]</span>
</div>
<p>Importantly, while in this case <span class="math inline">\(E(X)\)</span> corresponds well to the “peak” in the PMF, this should not be assumed to be the case unilaterally, as the same expectation would result from any distribution symmetrical about <span class="math inline">\(x=3\)</span>.</p>
</div>
<div id="conditional-probability" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Conditional Probability</h2>
<p>Conditional probability provides a way to model the probability that an event occurs, given that another event is known to have occured.
Conditional probability, as presented in Definition <a href="index.html#def:condprob">1.5</a>, requires only that the event assumed to have occured, i.e., the event we are <em>conditioning on</em> has a nonzero probability of occuring.</p>

<div class="definition">
<span id="def:condprob" class="definition"><strong>Definition 1.5  (Conditional Probability)  </strong></span><em>Assuming <span class="math inline">\(P(B)&gt;0\)</span>,</em>
<span class="math display">\[P(A|B)=\frac{P(A \cap B)}{P(B)}\]</span>
</div>
<p>Conditional probability asserts that the probability of event A occuring given that event B occurs is equivalent to the probability of both A and B occuring divided by the probability that B occurs. <span class="math inline">\(P(A|B)\)</span> is not generally equal to <span class="math inline">\(P(B|A)\)</span>
For example, the probability that I am swimming given that I am in the water is clearly not the same as the probability that I am in the water given that I am swimming.
Example <a href="index.html#exm:condprobe">1.2</a> explains how to use conditional probability to calculate the probability of rolling a 2 on a 6-sided dice, given I know the outcome of the roll is less than 4.</p>

<div class="example">
<span id="exm:condprobe" class="example"><strong>Example 1.2  </strong></span>Conditional probability can be used to determine the probability of rolling a 2 on a 6 sided dice, given that I know the outcome will be less than 4. This scenario can be represented as:
<span class="math display">\[A = \textrm{rolls 2, } P(A) = 1/6\]</span>
<span class="math display">\[B = \textrm{rolls &lt; 4, } P(B) = 4/6\]</span>
<span class="math display">\[\begin{split}
P(2|&lt;4) = P(A|B) &amp; = \frac{P(A \cap B)}{P(B)} \\
  &amp; = \frac{\frac{1}{6}\frac{4}{6}}{\frac{4}{6}} \\
  &amp; = \frac{1}{6} = 0.\overline{1666}
\end{split}\]</span>
</div>
</div>
<div id="bayes-theorem-for-discrete-values" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Bayes’ Theorem for Discrete Values</h2>
<p>Consider a student, Alice, who was exposed to someone with COVID-19.
Being a responsible person, Alice decides she should get tested.
She recieves a test and the accompanying information sheet states the test is 85% accurate, meaning that 85% of the time it gives positive results to recipients who are actually positive. The sheet also says that the test yields a false positive 30% of the time, meaning that if Alice is actually negative she will still recieve a positive test 30% of the time.
The following day Alice recieves a positive test.
As a student of probability, Alice recognizes that the 85% accuracy statistic only means the conditional probability that she recieves a positive test given she is COVID positive (<span class="math inline">\(P(+ test|covid+)\)</span>) is 85%.
However, Alice is actually interested in the conditional probability that she is positive given she just tested positive, <span class="math inline">\(P(covid+|test+)\)</span>.
As stated in the previous section’s discussion of conditional probability, <span class="math inline">\(P(A|B) != P(B|A)\)</span>.
Bayes’ theorem, which follows intuitively from the theorem of conditional probability, provides this answer for Alice.
Conditional probability as presented in Definition <a href="index.html#def:condprob">1.5</a>, can be trivially reframed as:
<span class="math display">\[P(A \cap B)=P(A|B)P(B).\]</span>
Furthuremore, it can also be stated that
<span class="math display">\[P(B \cap A)=P(B|A)P(A)\]</span>
Clearly <span class="math inline">\(P(A \cap B)=P(B \cap A)\)</span>, as both terms can be used interchangeably to represent the intersection of two sets A and B.
This equivalency means that we can rewrite these equations as
<span class="math display">\[P(B|A)P(A)=P(A|B)P(B)\]</span>
Dividing both sides of this equivalency by <span class="math inline">\(P(B)\)</span> yields Bayes’ theorem, as formalized in Definition <a href="index.html#def:bayestheorem">1.6</a> <span class="citation">(<a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html#ref-Junker2003" role="doc-biblioref">Junker 2003</a>)</span>.</p>

<div class="definition">
<span id="def:bayestheorem" class="definition"><strong>Definition 1.6  (Bayes’ Theorem)  </strong></span><em>Assuming <span class="math inline">\(P(B)&gt;0\)</span> and <span class="math inline">\(P(A)&gt;0\)</span>,</em>
<span class="math display">\[P(B|A)=\frac{P(A|B)P(B)}{P(A)}\]</span>
</div>
<p>Additionally, the <em>law of total probability</em>, Definition <a href="index.html#def:totprob">1.7</a>, can be used to compute <span class="math inline">\(P(A)\)</span> for a discrete sample space <span class="math inline">\(S_{B}\)</span> <span class="citation">(<a href="bayesian-applications-in-biology-hybrid-species-distribution-modelling-with-gaussian-processes.html#ref-Wasserman2004" role="doc-biblioref">Wasserman 2004</a>)</span>.</p>

<div class="definition">
<span id="def:totprob" class="definition"><strong>Definition 1.7  (Law of Total Probability)  </strong></span><em><span class="math inline">\(B_{0},...,B_{k}\)</span> is a partition of a discrete sample space <span class="math inline">\(S_{B}\)</span>, and</em>
<span class="math display">\[P(A)=\sum_{i=1}^{k}P(A|B_{i})P(B_{i}).\]</span>
</div>
<p>Returning to our example, the partition of this sample space is <span class="math inline">\([B_{0}=covid+,\textrm{ } B_{1}=covid-]\)</span>, as Alice is either COVID positive or she is not.
The final piece of information needed is the prior probability <span class="math inline">\(P(B)\)</span>, which can be thought of as the likelihood of contracting covid from any given exposure.
Alice did some research and concluded this likelihood is 20%.
Given the information from the factsheet and Alice’s prior knowledge about the probability of contracting covid, Example <a href="index.html#exm:covid">1.3</a> shows how to evaluate Bayes’ theorem for Alice’s inquiry.</p>

<div class="example">
<span id="exm:covid" class="example"><strong>Example 1.3  </strong></span>Given the following information:
<span class="math display">\[P(test+|covid+)=0.85 \textrm{, } P(test+|covid-)=0.30 \textrm{, } P(covid+)=0.20 \textrm{, } P(covid-)=0.80\]</span>
We can represent the conditional probability that Alice is COVID positive given that she tested positive <span class="math inline">\(P(covid+|test+)\)</span> as
<span class="math display">\[\begin{split}
P(covid+|test+) &amp; = \frac{P(test+|covid+)P(covid+)}{P(test+|covid-)P(covid-)+P(test+|covid+)P(covid+)}\\
&amp; = \frac{(0.85)(0.20)}{(0.30)(0.80)+(0.85)(0.20)} \\
&amp; = \frac{0.17}{0.24+0.17} \\
&amp; = 0.41
\end{split}\]</span>
</div>
</div>
<div id="bayes-theorem-for-probability-distributions" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> Bayes’ Theorem for Probability Distributions</h2>
<p>While Bayes’ theorem can be correctly applied to discrete values, evaluating Bayes’ theorem for probability distributions as an alternative to discrete values will allow uncertainty to be represented in a natural manner.
In the previous example, we considered the prior probability that Alice contracts COVID in any given exposure to be an exact value of 0.20. Given that the virus that causes COVID-19 is not well understood at present moment, this value is clearly not an accurate representation of the uncertainty related to our prior knowledge as different sources may give varying values for this prior plausibility.
Considering Bayes’ theorem in terms of probability distributions will remove this assumption from the model in favour of a probability distribution, which represents a weighted range of all possible values of our parameter as highlighted in Definition <a href="index.html#def:bayesprob">1.8</a>.</p>

<div class="definition">
<span id="def:bayesprob" class="definition"><strong>Definition 1.8  (Bayes’ Theorem for Probability Distributions)  </strong></span><em>Assuming <span class="math inline">\(P(\theta)&gt;0\)</span> and <span class="math inline">\(P(X)&gt;0\)</span>,</em>
<span class="math display">\[P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)},\]</span>
</div>
<p>where <span class="math inline">\(P(\theta|X)\)</span> is the posterior probability, <span class="math inline">\(P(\theta)\)</span> is the prior probability distribution of the parameter of interest <span class="math inline">\(\theta\)</span>, <span class="math inline">\(P(X|\theta)\)</span> is the likelihood function or the probability of the data given <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(P(X)\)</span> is the marginal likelihood of the data <span class="math inline">\(X\)</span>.
In this case, the marginal likelihood <span class="math inline">\(P(X)\)</span> functions to normalize the posterior distribution and is evaluated for a continuous sample space as:
<span class="math display">\[P(X)=\int P(X|\theta)P(\theta)d\theta\]</span></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="approximation-algorithms-for-inference-on-distributions.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["manuscript.pdf", "manuscript.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
