--- 
title: "Bayesian Inference for Species Distribution Modelling with Gaussian Processes"
subtitle: "Isaac William Caruso"
author: |
  | Amherst College
  | Department of Computer Science
date: "`r Sys.Date()`"
output:
  bookdown::pdf_book:
    includes:
      in_header: preamble.tex
description: "Bayesian Inference for Species Distribution Modelling with Gaussian Processes"
documentclass: book
link-citations: yes
bibliography:
  - references.bib
  - packages.bib
site: bookdown::bookdown_site
biblio-style: apalike
fontsize: 12pt
linestretch: 1.7
geometry: "left=1in, right=1in, top=1in, bottom=1in"
---
```{r setup, include = FALSE}
library(tidyverse)
library(mosaic)
library(mdsr)
library(knitr)
library(kableExtra)
library(latex2exp)
library(extrafont)
library(rstan)
#font_import(pattern = "cmu*") 
loadfonts()
setwd("~/bayes-gp-sdm-manuscript/")
```
# Probability and Bayes’ theorem

This chapter provides an introduction to the probability concepts necessary to understand Bayesian inference. 
Simply put, Bayesian inference is a statistical technique for estimating a quantity of interest upon the observation of data, while explicitly incorporating prior knowledge or belief about that quantity of interest.
Before embarking on an exposition of Bayesian statistics, we must first gain a basic understanding of a few key elements of probability theory—random variables, cumulative distribution functions, probability functions/ distributions, expected value, and conditional probability.
This chapter then concludes by introducing Bayes' theorem and Bayesian inference, and demonstrating the steps for performing a simple Bayesian update.


## Random Variables

Imagine for a moment you are tossing a fair coin. 
There are many experiments you could perform by tossing a coin, but let us consider our quantity of interest to be the fraction of times our coin lands on a tails. 
It is clear that the number of tails, the outcome of our experiment, is dependent on the eventual realization of some random process.
A random variable $X$ is a variable whose value is dependent on the outcome(s) of a stochastic phenomenon. 
The realized value of $X$ is denoted as $x$. 

In the example of tossing a coin, where the data is a sequence of coin tosses, e.g., $[H, T, T, …]$, we define the random variable $X$ to be the number of tails. 
If we toss the coin twice, $X$ has three possible realized states, $x$, depending on the outcome of this stochastic experiment: $x = 0$, $x = 1$, or $x = 2$. 
Table \@ref(tab:rv) shows the probability that our random variable $X$ takes value $x$, some actual number of tails.

```{r rv, echo=FALSE, fig.align='center'}
my_df = cbind(x = c(0, 1, 2),
          `P(X = x)` = c(0.25, 0.5, 0.25))
my_df %>%
  knitr::kable(caption = "P(X = x) for two tosses", booktabs = TRUE, format="latex") %>%
  kable_styling(latex_options = c("hold_position"))#, table.attr = "style='width:40%;'") %>%   
  #kable_styling(position = "center")

```


The r.v. $X$ is is an example of a *discrete random variable*. 
Discrete random variables can only assume discrete values. 
To the contrary, continuous random variables are useful for describing continuous sample spaces.
For example, a continuous random variable may be used to represent the outcome of an experiment measuring blossoming heights of flowers, where the data is a sequence of observations of heights at which different flowers blossomed. 
In this case, the outcome of our blossoming experiment can be any of an infinite number of real values, thus is properly modeled by a continuous random variable. 


## Cumulative Distribution Functions

In the previous example we represented the probability of various outcomes of a coin toss experiment in a tabular format. 
Another way to represent this set of probabilities is as a *cumulative distribution function*.

```{definition, cdf, name="Cumulative distribution function 'CDF'"}
*The cumulative distribution function is defined as a function where $F_{X}\in[0, 1]$:*
$$F_{X}(x) \doteq P(X≤x)$$
  
```


The cumulative distribution function $F_{X}(x)$ simply represents the probability that a random variable $X$ takes a value less than or equal to $x$ for each possible input value of $x$. 
Figure \@ref(fig:cdfimg) depicts a graphical representation of the CDF for our coin tossing experiment.

(ref:cdffig) CDF for tossing a coin twice [@Wasserman2004]

```{r cdfimg, echo=FALSE, out.width="70%", fig.align="center", fig.cap='(ref:cdffig)'}
knitr::include_graphics("./images/cdf.png", auto_pdf=TRUE)

```

Here, for every value of $x$—the number of tails in two coin tosses—the probability that $X$ is equal to or less than this value is represented.
In this discrete example, we see that our CDF is represented by several non-decreasing discrete lines defined for all $x$. 
In the example of a continuous random variable, this function is a left-continuous, non-decreasing distribution also defined for all $x$.

## Probability Mass and Density Functions

The probability mass function and probability density function allow us to express probabilities of events over a sample space, and find their use with discrete and continuous random variables respectively. 
  
In the discrete setting, the probability mass function for a random variable $X$ yields the probability that $X$ takes a value for every possible value that $X$ can take.

```{definition, pmf, name="Probability mass function 'PMF'"}
*The probability function for a discrete random variable $X$—the probability mass function for $X$—is defined as a function*
$$f_{X}(x) \doteq P(X=x)$$
  
```

Here, the PMF has a few key attributes. 
Namely $P(X=x)>0$ for every $x$ in the sample space $S_{X}$ of $X$, and $\sum_{x \in S_{X}}f_{X}(x)=1$. 
With these features in mind, the probability mass function of $X$ follows logically from the cumulative distribution function of $X$ insofar as the CDF is the sum of the PMF for all $x_{i}≤x$, i.e., $$F_{X}(x) \doteq P(X≤x)=\sum_{x_i≤x}f_{X}(x_{i}).$$

In the case where the random variable $X$ is continuous, its PDF is defined as follows.

```{definition, pdf, name="Probability density function 'PDF'"}
*The probability function for a continuous random variable $X$—the probability density function for $X$—is defined as a function $f(x)$ where $a$ and $b$ are two real numbers such that $a ≤ b$, so*
$$P(a<X<b) \doteq \int_{a}^{b}f_X(x)dx.$$
  
```

In other words, the probability that the realized value $x$ of our continuous random variable $X$ is between two numbers $a$ and $b$ is equal to the integral of the probability density function of $x$ from $x = a$ to $x = b$. 
This formalization of the PDF $f_X(x)$ allows a natural comparison with the CDF $F_X(x)$ of $X$,

$$F_X(x) \doteq \int_{-\infty}^{x}f_X(x)dx.$$

Specifically, this implies that $F'_X(x)=f_X(x)$ for all differentiable points of $F_X$. 
In plain English this signifies that the derivative of the CDF is the PDF.

## Expectation

One of the final core statistical concepts necessary to approach Bayesian statistics on sure footing is the idea of expectation or expected value. 

```{definition, e, name="Expectation"}
*The expectation or expected value of a random variable X is*
$$E(X) \doteq \begin{cases}\sum_{x}xf_{X}(x) & \textrm{if }X\textrm{ is discrete}\\\int{xf_{X}(x)dx} & \textrm{if }X\textrm{ is continuous}\end{cases}$$

```

To return to a familiar example, consider a random variable $X$ to represent the number of tails in 6 coin tosses. Figure \@ref(fig:binom) depicts the PMF for $X$ using the *binomial distribution* $B(6,0.5)$ which represents the probability of observing a specific number of successes in a success-failure experiment. 

```{r binom, echo=FALSE, fig.show="hold", out.width="50%", fig.align='center', fig.cap="The binomial probability mass function for the 6 trial coin toss experiment"}
x <- seq(0,6,by = 1)
y <- dbinom(x,6,0.5)

binom <- as.data.frame(cbind(x = x, y = y))

binom %>% ggplot(aes(x = x, y = y)) +
  geom_point(size=3) +
  geom_smooth(method='loess', formula = y ~ x, se=FALSE, color = "black", linetype = "dashed")  +
  xlab('# of tails in 6 tosses (x)') + 
  ylab('Probability') +
  theme_bw() + 
  theme(text = element_text(size=15, family="CMU Serif")) +
  ggExtra::removeGrid()

```

In this case, the x-axis represents each possible outcome $x$ and the y-axis is the probability of that outcome. Table \@ref(tab:bino) presents the value of the binomial PMF for every $x \in X$. 


```{r bino, echo=FALSE, fig.align='center'}
x <- seq(0,6,by = 1)
y <- dbinom(x,6,0.5) 
y <- round(y, digits = 3)
binom <- as.data.frame(cbind(x = x, y = y))

binom %>%
  knitr::kable(caption = "P(X = x) for six tosses", booktabs = TRUE, linesep = "", format="latex") %>%
  kable_styling(latex_options = c("hold_position"))#, table.attr = "style='width:40%;'") %>%   
  #kable_styling(position = "center")

```

Computing $E(X)$ given the values in this table is demonstrated using the discrete case of Definition \@ref(def:e) in Example \@ref(exm:coin).

```{example, coin}
The expected value of $X \sim B(6, 0.5)$,
$$\begin{split}
E(X) & = \sum_{x}xf(x) \\
  & =  (0 \times 0.16)+(1 \times 0.094)+(2 \times 0.234)+(3 \times 0.312)\\
  & \;\;\;\; +(4 \times 0.234)+(5 \times 0.094)+(6 \times 0.016)\\
  & = 3
\end{split}$$
```

Importantly, while in this case $E(X)$ corresponds well to the "peak" in the PMF, this should not be assumed to be the case unilaterally, as the same expectation would result from any distribution symmetrical about $x=3$.

## Conditional Probability

Conditional probability provides a way to model the probability that an event occurs, given that another event is known to have occured.
Conditional probability, as presented in Definition \@ref(def:condprob), requires only that the event assumed to have occured, i.e., the event we are *conditioning on*, has a nonzero probability of occuring.

```{definition, condprob, name="Conditional Probability"}
*Assuming $P(B)>0$,*
$$P(A|B)=\frac{P(A \cap B)}{P(B)}$$
```

Conditional probability asserts that the probability of event A occuring given that event B occurs is equivalent to the probability of both A and B occuring (denoted as $A \cap B$) divided by the probability that B occurs. $P(A|B)$ is not generally equal to $P(B|A)$ 
For example, the probability that I am swimming given that I am in the water is clearly not the same as the probability that I am in the water given that I am swimming.
Example \@ref(exm:condprobe) explains how to use conditional probability to calculate the probability of rolling a 2 on a 6-sided dice, given I know the outcome of the roll is less than 4.


```{example, condprobe}
Conditional probability can be used to determine the probability of rolling a 2 on a 6 sided dice, given that I know the outcome will be less than 4. This scenario can be represented as:
$$A = \textrm{rolls 2, } P(A) = 1/6$$
$$B = \textrm{rolls < 4, } P(B) = 3/6$$
$$P(A \cap B) = P(A) = 1/6$$
$$\begin{split}
P(2|<4) = P(A|B) & = \frac{P(A \cap B)}{P(B)} \\
  & = \frac{\frac{1}{6}}{\frac{3}{6}} \\
  & = \frac{1}{3}
\end{split}$$
```

## Bayes’ Theorem for Point Probabilities

Consider a student, Alice, who was exposed to someone with COVID-19. 
Being a responsible person, Alice decides she should get tested. 
She receives a test and the accompanying information sheet states the test is 85% accurate, meaning that 85% of the time it gives positive results to recipients who are actually positive. The sheet also says that the test yields a false positive 30% of the time, meaning that if Alice is actually negative she will still receive a positive test 30% of the time. 
The following day Alice receives a positive test.
As a student of probability, Alice recognizes that the 85% accuracy statistic only means the conditional probability that she receives a positive test given she is COVID positive ($P(+ test|covid+)$) is 85%.
However, Alice is actually interested in the conditional probability that she is positive given she just tested positive, $P(covid+|test+)$.
As stated in the previous section's discussion of conditional probability, $P(A|B) \neq P(B|A)$.
Bayes' theorem, which follows intuitively from the theorem of conditional probability, provides this answer for Alice.
We can rewrite Definition \@ref(def:condprob) as 
$$P(A \cap B)=P(A|B)P(B).$$
Furthuremore, it can also be stated that
$$P(B \cap A)=P(B|A)P(A)$$
Clearly $P(A \cap B)=P(B \cap A)$, as both terms can be used interchangeably to represent the intersection of two sets A and B.
This equivalency means that we can rewrite these equations as
$$P(B|A)P(A)=P(A|B)P(B)$$
Dividing both sides of this equivalency by $P(B)$ yields Bayes' theorem, as formalized in Definition \@ref(def:bayestheorem) [@Junker2003]. 

```{definition, bayestheorem, name="Bayes' Theorem"}
*Assuming $P(B)>0$ and $P(A)>0$,*
$$P(B|A)=\frac{P(A|B)P(B)}{P(A)}$$
```

Additionally, the *law of total probability*, Definition \@ref(def:totprob), can be used to compute $P(A)$ for a discrete sample space $S_{B}$ [@Wasserman2004].

```{definition, totprob, name="Law of Total Probability"}
*$B_{0},...,B_{k}$ is a partition of a discrete sample space $S_{B}$, and*
$$P(A)=\sum_{i=1}^{k}P(A|B_{i})P(B_{i}).$$
```

Returning to our example, the partition of this sample space is $[B_{0}=covid+,\textrm{ } B_{1}=covid-]$, as Alice is either COVID positive or she is not. 
The final piece of information needed is the *prior* probability $P(B)$, which can be thought of as the likelihood of contracting covid from any given exposure. 
Alice did some research and concluded this likelihood is 20%. 
Given the information from the factsheet and Alice's prior knowledge about the probability of contracting covid, Example \@ref(exm:covid) shows how Alice can use Bayes' theorem to answer her question and find the probability that she is actually positive given she has tested positive.

```{example, covid}
Given the following information:
$$P(test+|covid+)=0.85 \textrm{, } P(test+|covid-)=0.30 \textrm{, } P(covid+)=0.20 \textrm{, } P(covid-)=0.80$$
We can represent the conditional probability that Alice is COVID positive given that she tested positive $P(covid+|test+)$ as
$$\begin{split}
P(covid+|test+) & = \frac{P(test+|covid+)P(covid+)}{P(test+|covid-)P(covid-)+P(test+|covid+)P(covid+)}\\
& = \frac{(0.85)(0.20)}{(0.30)(0.80)+(0.85)(0.20)} \\
& = \frac{0.17}{0.24+0.17} \\
& = 0.41
\end{split}$$
```

## Bayesian Inference

In the previous example, we used Bayes' theorem to evaluate the probability that Alice is COVID positive given she recieved a positive test. 
Bayes' theorem provided a way for Alice to apply her prior beliefs about the likelihood of actually being positive to the data she observed---a positive test. 
*Learning*, or *statistical inference*, asks a generalized version of Alice's question. 
Given some data $X_0,...,X_i \sim D$, how can the distribution $D$ that generated the data $X_0,...,X_i$ be inferred [@Wasserman2004 \text{Sec} 6.1]?
Additionally as distributions are parameterized by some parameters $\theta_0, ..., \theta_j$, estimating these parameters given the data is one avenue for inferring $D$.
*Bayesian inference* applies Bayes' theorem to the task of statistical inference in an intuitive manner, which by definition maintains the inherent uncertainty in inference.
Contrary to the frequentist perspective, a Bayesian approach to inference expresses degrees of belief by producing probability distributions for desired parameters $\theta_0, ..., \theta_j$ of the distribution [@Wasserman2004 Sec 11.1]. 
Point estimates and confidence intervals can then be computed with a post hoc analysis of the posterior distribution.
Furthuremore, a Bayesian approach to inference explicitly includes previous domain specific knowledge (or lack thereof) in the iterative learning process.
Performing Bayesian inference to compute a posterior distribution $P(\theta|X)$ for a parameter $\theta$ first requires choosing a PDF $f(\theta)$ with a *prior probability distribution* $P(\theta)$ which expresses our prior beliefs about $\theta$ before observing any data. 
In the previous example, this was Alice's previous belief about the probability of contracting COVID from any given exposure.
Alternatively, experessing $P(\theta)$ as a distribution allows for the inclusion of the inherent uncertainty of Alice's preconceptions in the modelling schema and thus the computation of the posterior.
Secondly, the likelihod function $l(X|\theta)$ must be specified to express our understanding of the probability of the data given a particular value of $\theta$.
In the case of Alice's inquiry, a binomial likelihood function would be appropriate as it represents the likelihood of observing a given number of successes ($+$tests) in $0 \lor 1$ ($+ \lor -$) trials.
The final step in performing Bayesian inference is repeatedly applying Bayes' theorem to update the probability distribution of $\theta$ for each observation $X_n$ in the data $n \in (0,i)$, resulting in a posterior distribution $f(\theta|X_0,...,X_i)$.
This is done by iteratively evaluating Bayes' theorem, where the posterior for the $n-1^{th}$ iteration is the prior for the $n^{th}$ iteration.
Bayesian inference relies on Bayes' theorem for continuous variables, which utilizes density functions as presented in formally Definition \@ref(def:bayesprob).

```{definition, bayesprob, name="Bayes' Theorem for Continuous Variables"}
*Assuming $f(\theta)>0$ and $f(X)>0$,*
$$f(\theta|X)=\frac{l(X|\theta)f(\theta)}{f(X)},$$
```
where $f(\theta|X)$ is the posterior PDF, $f(\theta)$ is the prior PDF of $\theta$, $l(X|\theta)$ is the likelihood function or the probability of the data given $\theta$, and $f(X)$ is the marginal likelihood of the data $X$.
In this case, the marginal likelihood $f(X)$ functions only to normalize the posterior distribution and is evaluated for a continuous sample space as:
$$f(X)=\int l(X|\theta)f(\theta)d\theta \textrm{,}$$
meaning that intuitively, the posterior is proportional to the likelihood times the prior.

While Bayesian inference is an appealing learning schema for a number of reasons, modelling real-world systems using this technique requires the use of approximation algorithms to be computationally feasible.
As previously mentioned, depending on the system we may be interested in evaulating an arbitrary number parameters $\theta_0, ..., \theta_j$ given the data, and each parameter may potentially take on an infinite number of values.
Even in the case where we only consider a small number of possible values $a$ for every $\theta_i \in (0,j)$, the evaluation of the posterior to perform a single update rapidly becomes a computationally intractible task.
This is evidenced by the runtime, which is proportional to $O(a^{j})$.
In order to perform Bayesian inference on complex systems, fast algorithms are necessary to circumnavigate this curse of dimensionality.

# Algorithms for Bayesian inference

When performing Bayesian inference, fast algorithms are necessary for updating the probability distributions of parameters of interest in a computationally feasible manner.
In modelling scenarios with multiple parameters forming a multidimensional parameter space, evaluating parameters at even a relatively small number of possible values rapidly becomes intractible, as a product of the curse of dimensionality.
This problem is further exaggerated in the evaluation of hierarchical models, which are of particular interest in many application domains such as biology, economics, chemistry, and physics.
Here, we discuss Markov Chain Monte Carlo (MCMC) based sampling algorithms, which allow for efficiently sampling from approximations of probability distributions.

## Markov Chain Monte Carlo

Markov Chain Monte Carlo (MCMC) provides an efficient methodology for sampling from the posterior curve to perform a Bayesian update. We first provide a theoretical background into Markov chains and Monte Carlo approximation, followed by a discussion of *Metropolis-Hastings*, an algorithm for performing MCMC. 

### Markov Chains

A Markov chain is a stochastic model expressing a sequence of possible states in which the probability of each state, $X_{i}$, depends only on the value attained in the previous state, $X_{i-1}$, for all $i \geq 0$.
A Markov chain has several properties which are essential for its application in Bayesian statistics. 
Definition \@ref(def:markov) formalizes the requirements for a Markov chain [@Wasserman2004].

```{definition, markov, name="Markov Chain"}
*A discrete sequence of random variables ${X_{0},X_{1},...,X_{i}}$ is a Markov chain iff it satisfies the Markov property; that is, for all $i$ and $x \in X$:*
$$P(X_{i}=x|X_{0},...X_{i-1})=P(X_{i}=x|X_{i-1})$$
```

A Markov chain is often represented as a directed, weighted graph where possible states are represented as vertices, and edges represent possible transitions between states. The edge weights are the conditional probabilities of the next state of the chain being the one corresponding to the edge target vertex, given that the correct state is the one corresponding to the edge source vertex. These probabilities are also known as transition probabilities.

### Monte Carlo Approximation

Monte Carlo approximation provides a convient method for approximately computing quantities of interest.
This schema will allow for drawing samples from arbitrarily complex probability distributions.
The basic example is Monte Carlo integration [@Wasserman2004 \text{Sec} 24.2]. 
If we want to evaulate an integral for some function $f(x)$, where 
$$I=\int_{a}^{b}f(x)dx \textrm{ ,}$$
we can approximate $I$ using Monte Carlo approximation. $f(x)$ can be alternatively expressed as two functions, $h(x)$ and $w(x)$, where $h(x)=\frac{1}{a-b}$ and $w(x)=f(x)(b-a)$ as follows.
$$I=\int_{a}^{b}f(x)dx=\int_{a}^{b}w(x)h(x)dx$$
Conveniently, $h$ is the pdf of a uniform r.v. $X$ over $[a,b]$, which means that $I$ can be rewritten in terms of expectation as
$$I=E_{f}(w(X)) \textrm{ .}$$
In conjunction with the law of large numbers, this means that if we generate a sequence of random variables from a uniform distribution $X_{0},...,X_{N}\sim unif(a,b)$ then the standard Monte Carlo integration method asserts

$$\hat{I}\equiv\frac{1}{N}\sum_{i=0}^{N}w(X_{i})\rightarrow E(w(X))=I \textrm{ .}$$

In other words, $\hat{I}$ approaches $I$ as $N$ grows sufficiently large. 
As we will expand on in the following sections, this algorithmic framework is particularly useful because it will work for models with non-normal posteriors, including hierarchical models. 

## Metropolis-Hastings MCMC

*Metropolis-Hastings MCMC* is one of the most used algorithms for sampling from the posterior distribution to perform a Bayesian update. 
While it is not always the fastest in practice and requires manual tuning to work effectively, Metropolis-Hastings provides a foundation for more complex algorithms such as *Hamiltonian Monte Carlo* [@Brooks2011] with *No-U-Turn sampling* [@Homan2014]. 
To reiterate, the purpose of sampling in Bayesian inference is to draw from some density for parameters $\theta$. 
In this case, the density we are drawing from is the Bayesian posterior $P(\theta|data)$, which for the purposes of MCMC is referred to as the *target distribution*. 
The Metropolis-Hastings algorithm provides a method for sampling from an approximation of the target distribution known as the *stationary distribution*. 
Critically, the law of large numbers guarantees that with sufficiently many iterations of Metropolis-Hastings, the generated sequence of states will converge on the stationary distribution.
Essentially, given some sequence of states $X_{0},X_{1},...,X_{i}$ from a Markov-Chain, where $X_{0}$ is chosen arbitrarily, an iteration of Metropolis-Hastings produces the next state to include in the sequence. 
As presented formally in Definition \@ref(def:methas), this is achieved by generating a *proposal* for $X_{i+1}$ from the *proposal distribution* $q(y|X_{i})$, transition kernel, and then accepting the proposal with some probability dependent on the relative, target probabilities of the current state $X_{i}$ and the proposal [@Wasserman2004].

```{definition, methas, name="Metropolis-Hastings MCMC"}
*$X_{i+1}$ is generated given $X_{0},X_{1},...,X_{i}$ in the following manner:*
\begin{enumerate}
  \item Sample a proposal $Y$ from the proposal distribution $Y \sim q(y|X_{i}) \textrm{ .}$
  \item Evaulate the ratio of probabilities in the stationary distribution $h(x)$ for $r(X_{i},Y)$ where $$ r(x,y)=min \{\frac{f(y)}{f(x)} \frac{q(x|y)}{q(y|x)} ,1 \} \textrm{ .}$$
  \item Compute the next state $X_{i+1}$, where $$X_{i+1}=\begin{cases}Y & \textrm{with probability }r \\ X_{i} & \textrm{with probability }1-r \end{cases} \textrm{ .}$$
\end{enumerate}
```

While Metropolis-Hastings may seem appealing because it is both memoryless and able to approximate very complex distributions, its downfall lies in the manual tuning necessary to achieve convergence of the stationary distribution to the target distribution in a reasonable number of iterations. 
Recall that Metropolis-Hastings generates proposals by sampling from some distribution.
In the case of random walk Metropolis-Hastings, this is a normal distribution centered about $X_{i}$. 
Here, the standard deviation of the normal distribution, known as the step size, dictates the relative distance in the sample space between $X_{i}$ and the generated proposal.
If the step size is too low, the algorithm will make very small steps and may miss key features of the target distribution, causing the stationary distribution to require a much larger number of iterations to converge on the target distribution.
On the other hand if the step size is too large, proposals will be overwhelmingly generated from the low-probability tails of the distribution, again resulting in a lack of convergence and poorly representative samples from the stationary distribution.
While some methods have been proposed for automatically tuning the step size parameter [@Graves2011], iterating on complex, Bayesian models to tune a parameter is not particularly efficient nor computationally feasible in many cases. 
For this reason, other approximation algorithms are used in practical implementations of Bayesian inference.
One such algorithm, *Hamiltonian Monte Carlo* (HMC), relies on theorhetical physics to compute a latent momentum variable which is applied to a hamiltonian, effectively simulating a ball rolling around the multi-dimensional sample space [@Brooks2011]. 
Despite the increased computational cost, this method is appealing because of its ability to generate proposals from distant regions of the stationary distribution with high acceptance probabilities.
This means that in practice, the HMC algorithm's stationary distribution converges on the target distribution with far fewer iterations than traditional Metropolis-Hastings implementations. 
Though HMC still requires the user to specify a step size as well as a number of steps to move the hamiltonian before considering a proposal, in practice it is much more efficient and requires fewer iterations on a model.
Additionally, a proposed extension to HMC called the *No-U-Turn Sampler* (NUTS) automatically determines the number of steps and was empirically demonstrated to perform at least equally as well as standard HMC [@Homan2014].
The approximation algorithms breifly covered in this chapter underpin modern applications for Bayesian inference, including Stan, which are discussed in the following chapter.

# Modern Bayesian inference with Stan

Co-founded at Columbia University by Bob Carpenter and Andrew Gelman, Stan is a domain specific programming language that provides tools for specifying, executing and evaluating statistical models.
Currently comprised of more than 45 core developers from a wide array of educational institutions, private entities, and research organizations, over the past 9 years Stan has grown to become one of the leading options for performing Bayesian inference computationally [@Stan2021].
Among the many capabilities of the Stan language, is the ability to define priors, specify dependencies between variables, and impose known limits on parameters.
After all model components have been specified, a Stan model compiles to C++ and can then be executed to produce a fit.

## The Stan Program

While Stan is not strictly a tool for Bayesian statistical methods, it is most often used for its efficient MCMC sampler for Bayesian inference.
In essence, Stan has a sampler that takes a model and creates a separate program which explores the sample space to learn the parameters of interest in the modelling task using what is essentially the previously mentioned NUTS HMC sampling algorithm to generate draws from the poseterior distribution.
As it is written in C++ and thus produces compiled code, a Stan program is able to take advantage of the full capabilities of the machine on which it is executed.
Theoretically, Stan allows computer scientists and statisticians to construct, evaluate, and iterate on extremely complex, hierachical models in one consolidated environment.

A typical Stan program is comprised of several, optional *program blocks*, which provide a general organizational structure to a Stan program and generally allow for the declaration of variables and accompanying statements about those variables.
Block types include functions, data, transformed data, parameters, transformed prarameters, model, and generated quantities. 
Each of these block types serve a different purpose in a general modelling schema; however no block is required for the Stan compiler to execute, and an empty string is considered a valid Stan program (though it may not produce a very informative model).
After the program samples the user specified number of iterations and completes execution, a fit object and accompanying summary statistics are produced, which allow for performing post-hoc evaluation such as the posterior predictive check. 
```{r include=FALSE, cache=TRUE}
sharks = read.csv('./data/florida-shark.csv')
```
```{css, echo=FALSE}
.small {
  font-size: small;
}
```

```{example, stan}
Bob, a marine biologist at Florida Fish and Wildlife, studys shifting patterns in encounters between humans and large marine wildlife.
After seeing a local news headline about a recent string of shark attacks in his town, Bob became interested in trying to determine if the rate of shark attacks from year to year in Florida is changing. To do this, Bob decides to use Bayesian inference to compute a Poisson regression for a dataset of shark attack counts each year in Florida [@Collier2018]. Bayesian poisson regression models are useful for learning parameters that take the form of a count per unit time or space. Specifically, in a poisson regression scenario, the data $X_0,...,X_n$ are Poisson($\lambda$), thus the prior distribution (the conjugate prior of the Poisson) is a gamma distribution with shape $\alpha$ and rate $\beta$ parameters [@Hitchcock2014]. The data takes the following form:
```
```{r echo=FALSE,  comment=''}
glimpse(sharks, width=80)
```
Bob writes the model in stan with three blocks---data, parameters, and model. 
```{Stan, class.source="small", class.output="small", output.var="ex1", size='small', eval=FALSE, collapse=TRUE, cache=TRUE, strip.white=TRUE}
data {
    int<lower=0> N;              # Number of observations in dataset
    int<lower=0> attacks[N];     # An array of attack counts
    real         year[N];        # A corresponding array of years
    int<lower=0> population[N];} # Array of population for each year
parameters {
    real alpha;                  # Shape parameter
    real beta;}                  # Rate parameter
model {
    beta ~ normal(0.015, 0.015); # Placing a normal prior on beta
    for (n in 1:N) # Every count modelled by a log-link with a linear combination
        attacks[n] ~ poisson_log(log(population[n]) + alpha + beta * year[n]);}
```
Here, the data block is used to specify the type and quantity of data to be processed by the model. While $N$ could be replaced with an actual integer, keeping fields of this type allows for models to be more flexible and require less changes. Additionally, the parameters block is used to declare the model's parameters, which correspond directly to the variables Stan will sample at run time [@StanManual2016  \text{Sec} 8.1]. Finally, the model parameter is used to define the model, which includes the specification of any custom priors and the overall model formulae. In this case, Bob put a $normal(0.015,0.015)$ prior on beta to include into the model his domain specific assumption that shark attacks are occuring at increasing rates each year---specifically, that this rate is increasing at about 1.5% each year. After specifying the model, Bob uses the Stan compiler to compile his code and then executes it to sample his model. After his model has finished sampling, Bob first takes a look at the summary statistics.
```{r include=FALSE, cache=TRUE}
set.seed(500)
sharks = read.csv('./data/florida-shark.csv')
sharks_stancode = "data {
    int<lower=0> N;
    int<lower=0> attacks[N];
    real         year[N];
    int<lower=0> population[N];
}
parameters {
    real alpha;
    real beta;
}
model {
    beta ~ normal(0.015, 0.015);
    for (n in 1:N)
        attacks[n] ~ poisson_log(log(population[n]) + alpha + beta * year[n]);
}
generated quantities {
    real attacks_pred[N];
    for (n in 1:N)
        attacks_pred[n] = poisson_log_rng(log(population[n]) + alpha + beta * year[n]);
}"

sharks_data = list(
    'N' = length(sharks$population),
    'population'= sharks$population,
    'year'= sharks$year,
    'attacks' = sharks$attacks
)
fit = stan(model_code = sharks_stancode, cores=4, data=sharks_data, chains=4, iter=2000)
```
```{r, cache=TRUE, class.output="small", comment='', tidy=TRUE, size='footnotesize', tidy.opts = list(blank = FALSE, width.cutoff = 30), echo=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=40), tidy=TRUE)
print(summary(fit, pars=c('alpha', 'beta'))$summary, digits=2)
```
Bob notes that beta, the rate parameter he is interested in understanding, seems to be increasing by nearly 3% each year, although the 97.5% confidence interval of 3.5% causes Bob to wonder how certain he should be in his conclusion about beta. To get a better understanding of the relative frequencies of different samples of the model's parameter values, Bob plots the sample values for each iteration on a histogram, as presented in Figure \@ref(fig:hist).
```{r hist, cache=TRUE, comment='', fig.height=3, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Histograms displaying frequencies of varying sampled values for parameters alpha and beta"}
stan_hist(fit, pars=c('alpha', 'beta')) +#, fill="grey") +
  theme_mdsr() +
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    plot.title = element_blank()) +
  labs(xaxis = c('alpha', 'beta'))
```
The final analysis step for Bob's Bayesian model is ensuring the chains mixed properly, which Bob determines qualitatively using a trace plot as shown in Figure \@ref(fig:trace). 
```{r trace, cache=TRUE, comment='', fig.height=2, echo=FALSE, warning=FALSE, fig.align='center', fig.cap="Each chain represents a sequence of states sampled from the posterior for parameters alpha and beta. Four chains were sampled for this model, and this trace plot is a qualitative method for determining if the sample space for each parameter was sufficiently explored. Here, the sampled value of each parameter (y-axis) is plotted for every iteration Stan evaluated. In this case, Bob specified 2000 total iterations and 1000 burn in iterations, so the first 1000 samples were discarded and not included in the posterior or the trace."}
stan_trace(fit, pars=c('alpha', 'beta')) +
  theme_mdsr() +
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank())
```
As it is clear the chains mixed well, Bob is now relatively confident in the results his model is producing. While he does trust the samples evaluated by his model and his analysis of Figure \@ref(fig:hist)---that the rate of shark attacks is indeed increasing in Florida by some amount---the general lack of concentrated density in both histograms leads Bob to determine that he should collect more data and continue sampling his fit in an attempt to gain a better understanding of preciesly how much shark attacks are increasing each year.
Stan, and more generally Bayesian inference, is a good option for a task like this, because Bob can simply collect more data and continue training this model with his new data, without the need to start the fitting process over again.

## Why Stan?

According to Andrew Gelman, a well known Bayesian statistician at Columbia University, and associates the motivation behind devloping STAN was to addess several shortcomings in other technologies for performing Bayesian inference, which made these alternatives impractical for learning large, complex systems [@Gelman2015].
The four main shortcomings of previous software Stan's developers aimed to address were flexibility (being able to fit any given model), ease of use/ user programming time, run time, and scalability to larger datasets and more complex models [@Gelman2015].
Compared to two of its most direct predecessors, Jags and Bugs, STAN is generally more flexible although notably it cannot handle discrete parameters while both Jags and Bugs can.
Stan was demonstrated to be both faster and scale better for complex models than Bugs and Jags, which the developers contribute to general efficiency in implementation, use of memory management, and most importantly the advanced MCMC algorithm STAN employs [@Gelman2015].
Stan's use of HMC [@Brooks2011] and a slightly modified No-U-Turn sampler [@Homan2014], means that it often provides large efficiency boosts compared to conventional solutions using Metropolis-Hastings or Gibbs sampling (Bugs and Jags).  
In one example, the authors demonstrated that Stan can sample a hierarchical logistic regression model of a large dataset approximately twice as fast as Jags and produce an effective sample size of nearly four times its predecessor [@Gelman2015].
While it is highly performant compared to previous options, Stan's overall performance is strongly dependent on the joint posterior of all parameters, and as we will elaborate upon in following chapters can struggle to sample big datasets with numerous *gaussian processes* [@Gelman2015].


# Bayesian applications in Biology: Hybrid species distribution modelling with gaussian processes




```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```
